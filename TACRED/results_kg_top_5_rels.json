{
"run_1":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9787985865724381,
                "recall": 0.9964028776978417,
                "f1-score": 0.9875222816399288,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.9512195121951219,
                "f1-score": 0.975,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9782608695652174,
                "recall": 0.9926470588235294,
                "f1-score": 0.9854014598540147,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9872881355932204,
            "macro avg": {
                "precision": 0.9914118912275309,
                "recall": 0.9859262301688305,
                "f1-score": 0.9885152295822112,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9874992730621572,
                "recall": 0.9872881355932204,
                "f1-score": 0.9872431059876481,
                "support": 708
            }
        }
    
},
"run_2":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9963898916967509,
                "recall": 0.9928057553956835,
                "f1-score": 0.9945945945945946,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.991869918699187,
                "f1-score": 0.9959183673469388,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9712230215827338,
                "recall": 0.9926470588235294,
                "f1-score": 0.9818181818181818,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9929378531073446,
            "macro avg": {
                "precision": 0.993522582655897,
                "recall": 0.9933368870092119,
                "f1-score": 0.9933967100353656,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9930546904335431,
                "recall": 0.9929378531073446,
                "f1-score": 0.9929658974738024,
                "support": 708
            }
        }
    
},
"run_3":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9857142857142858,
                "recall": 0.9928057553956835,
                "f1-score": 0.989247311827957,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.967479674796748,
                "f1-score": 0.9834710743801653,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9712230215827338,
                "recall": 0.9926470588235294,
                "f1-score": 0.9818181818181818,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9887005649717514,
            "macro avg": {
                "precision": 0.9913874614594039,
                "recall": 0.988458838228724,
                "f1-score": 0.9898377948886832,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9888628564460781,
                "recall": 0.9887005649717514,
                "f1-score": 0.988703804756234,
                "support": 708
            }
        }
    
},
"run_4":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9892086330935251,
                "f1-score": 0.9945750452079565,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.991869918699187,
                "recall": 0.991869918699187,
                "f1-score": 0.991869918699187,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9714285714285714,
                "recall": 1.0,
                "f1-score": 0.9855072463768115,
                "support": 136
            },
            "per:age": {
                "precision": 0.9893617021276596,
                "recall": 0.9893617021276596,
                "f1-score": 0.9893617021276596,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 0.987012987012987,
                "f1-score": 0.9934640522875817,
                "support": 77
            },
            "accuracy": 0.9915254237288136,
            "macro avg": {
                "precision": 0.9905320384510837,
                "recall": 0.9914906481866718,
                "f1-score": 0.9909555929398394,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.991686844229217,
                "recall": 0.9915254237288136,
                "f1-score": 0.9915502543802289,
                "support": 708
            }
        }
    
},
"run_5":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9928057553956835,
                "f1-score": 0.996389891696751,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.991869918699187,
                "recall": 0.991869918699187,
                "f1-score": 0.991869918699187,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9943502824858758,
            "macro avg": {
                "precision": 0.9940574369772474,
                "recall": 0.994807475244506,
                "f1-score": 0.9944006251807919,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9944417347477951,
                "recall": 0.9943502824858758,
                "f1-score": 0.9943645193944476,
                "support": 708
            }
        }
    
},
"run_6":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9856115107913669,
                "f1-score": 0.9927536231884058,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.9838709677419355,
                "recall": 0.991869918699187,
                "f1-score": 0.9878542510121457,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9714285714285714,
                "recall": 1.0,
                "f1-score": 0.9855072463768115,
                "support": 136
            },
            "per:age": {
                "precision": 0.9893617021276596,
                "recall": 0.9893617021276596,
                "f1-score": 0.9893617021276596,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 0.987012987012987,
                "f1-score": 0.9934640522875817,
                "support": 77
            },
            "accuracy": 0.9901129943502824,
            "macro avg": {
                "precision": 0.9889322482596334,
                "recall": 0.9907712237262402,
                "f1-score": 0.989788174998521,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9902971959696945,
                "recall": 0.9901129943502824,
                "f1-score": 0.9901374260653403,
                "support": 708
            }
        }
    
},
"run_7":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9584775086505191,
                "recall": 0.9964028776978417,
                "f1-score": 0.9770723104056438,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.8943089430894309,
                "f1-score": 0.944206008583691,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9788135593220338,
            "macro avg": {
                "precision": 0.9873789549675138,
                "recall": 0.9760147045829864,
                "f1-score": 0.9810043268994711,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9795501350371231,
                "recall": 0.9788135593220338,
                "f1-score": 0.9784987728645977,
                "support": 708
            }
        }
    
},
"run_8":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9892086330935251,
                "f1-score": 0.9945750452079565,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.9838709677419355,
                "recall": 0.991869918699187,
                "f1-score": 0.9878542510121457,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9929378531073446,
            "macro avg": {
                "precision": 0.9924576467857971,
                "recall": 0.9940880507840744,
                "f1-score": 0.9932345223456247,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9930520864882725,
                "recall": 0.9929378531073446,
                "f1-score": 0.9929542729970028,
                "support": 708
            }
        }
    
},
"run_9":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9963503649635036,
                "recall": 0.9820143884892086,
                "f1-score": 0.9891304347826086,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.9682539682539683,
                "recall": 0.991869918699187,
                "f1-score": 0.9799196787148594,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9782608695652174,
                "recall": 0.9926470588235294,
                "f1-score": 0.9854014598540147,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9887005649717514,
            "macro avg": {
                "precision": 0.9885730405565379,
                "recall": 0.9911786136279168,
                "f1-score": 0.9898207959537191,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9888758726214147,
                "recall": 0.9887005649717514,
                "f1-score": 0.9887292458966772,
                "support": 708
            }
        }
    
},
"run_10":
{
    "model": "IEModelGoldKG",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9787985865724381,
                "recall": 0.9964028776978417,
                "f1-score": 0.9875222816399288,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.9512195121951219,
                "f1-score": 0.975,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9782608695652174,
                "recall": 0.9926470588235294,
                "f1-score": 0.9854014598540147,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9872881355932204,
            "macro avg": {
                "precision": 0.9914118912275309,
                "recall": 0.9859262301688305,
                "f1-score": 0.9885152295822112,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9874992730621572,
                "recall": 0.9872881355932204,
                "f1-score": 0.9872431059876481,
                "support": 708
            }
        }
    
}}
