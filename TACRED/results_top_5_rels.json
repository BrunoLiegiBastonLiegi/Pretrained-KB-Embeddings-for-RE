{
"run_1":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9892857142857143,
                "recall": 0.9964028776978417,
                "f1-score": 0.992831541218638,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.967479674796748,
                "f1-score": 0.9834710743801653,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9915254237288136,
            "macro avg": {
                "precision": 0.993540596094553,
                "recall": 0.9906488509244499,
                "f1-score": 0.9920091862213649,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9916471423345585,
                "recall": 0.9915254237288136,
                "f1-score": 0.9915081927219122,
                "support": 708
            }
        }
    
},
"run_2":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9964028776978417,
                "recall": 0.9964028776978417,
                "f1-score": 0.9964028776978417,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.983739837398374,
                "f1-score": 0.9918032786885246,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9943502824858758,
            "macro avg": {
                "precision": 0.9949640287769785,
                "recall": 0.9939008834447751,
                "f1-score": 0.9943898943788774,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9944417347477951,
                "recall": 0.9943502824858758,
                "f1-score": 0.9943580411274868,
                "support": 708
            }
        }
    
},
"run_3":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9928057553956835,
                "f1-score": 0.996389891696751,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.991869918699187,
                "recall": 0.991869918699187,
                "f1-score": 0.991869918699187,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9943502824858758,
            "macro avg": {
                "precision": 0.9940574369772474,
                "recall": 0.994807475244506,
                "f1-score": 0.9944006251807919,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9944417347477951,
                "recall": 0.9943502824858758,
                "f1-score": 0.9943645193944476,
                "support": 708
            }
        }
    
},
"run_4":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.996268656716418,
                "recall": 0.960431654676259,
                "f1-score": 0.978021978021978,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.9384615384615385,
                "recall": 0.991869918699187,
                "f1-score": 0.9644268774703556,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9712230215827338,
                "recall": 0.9926470588235294,
                "f1-score": 0.9818181818181818,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 0.9871794871794872,
                "recall": 1.0,
                "f1-score": 0.9935483870967742,
                "support": 77
            },
            "accuracy": 0.980225988700565,
            "macro avg": {
                "precision": 0.9786265407880356,
                "recall": 0.9868620668653272,
                "f1-score": 0.9824935661648804,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9809217616469007,
                "recall": 0.980225988700565,
                "f1-score": 0.9802859329885544,
                "support": 708
            }
        }
    
},
"run_5":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9719298245614035,
                "recall": 0.9964028776978417,
                "f1-score": 0.9840142095914742,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.926829268292683,
                "f1-score": 0.9620253164556963,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9844632768361582,
            "macro avg": {
                "precision": 0.9900694181496907,
                "recall": 0.9825187696236369,
                "f1-score": 0.9859565683110383,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9848322590812274,
                "recall": 0.9844632768361582,
                "f1-score": 0.9843202740537468,
                "support": 708
            }
        }
    
},
"run_6":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9892857142857143,
                "recall": 0.9964028776978417,
                "f1-score": 0.992831541218638,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.967479674796748,
                "f1-score": 0.9834710743801653,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9915254237288136,
            "macro avg": {
                "precision": 0.993540596094553,
                "recall": 0.9906488509244499,
                "f1-score": 0.9920091862213649,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9916471423345585,
                "recall": 0.9915254237288136,
                "f1-score": 0.9915081927219122,
                "support": 708
            }
        }
    
},
"run_7":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.992831541218638,
                "recall": 0.9964028776978417,
                "f1-score": 0.9946140035906642,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.975609756097561,
                "f1-score": 0.9876543209876543,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9929378531073446,
            "macro avg": {
                "precision": 0.9942497614811376,
                "recall": 0.9922748671846126,
                "f1-score": 0.9932023280172679,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9930394303110456,
                "recall": 0.9929378531073446,
                "f1-score": 0.9929348366091219,
                "support": 708
            }
        }
    
},
"run_8":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9892857142857143,
                "recall": 0.9964028776978417,
                "f1-score": 0.992831541218638,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.967479674796748,
                "f1-score": 0.9834710743801653,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9915254237288136,
            "macro avg": {
                "precision": 0.993540596094553,
                "recall": 0.9906488509244499,
                "f1-score": 0.9920091862213649,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9916471423345585,
                "recall": 0.9915254237288136,
                "f1-score": 0.9915081927219122,
                "support": 708
            }
        }
    
},
"run_9":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 1.0,
                "recall": 0.9928057553956835,
                "f1-score": 0.996389891696751,
                "support": 278
            },
            "per:employee_of": {
                "precision": 0.991869918699187,
                "recall": 0.991869918699187,
                "f1-score": 0.991869918699187,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9943502824858758,
            "macro avg": {
                "precision": 0.9940574369772474,
                "recall": 0.994807475244506,
                "f1-score": 0.9944006251807919,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9944417347477951,
                "recall": 0.9943502824858758,
                "f1-score": 0.9943645193944476,
                "support": 708
            }
        }
    
},
"run_10":
{
    "model": "BaseIEModelGoldEntities",
    "learning_rate": 2e-05,
    "epochs": 6,
    "batchsize": 8,
    "scores": 
        {
            "per:title": {
                "precision": 0.9964028776978417,
                "recall": 0.9964028776978417,
                "f1-score": 0.9964028776978417,
                "support": 278
            },
            "per:employee_of": {
                "precision": 1.0,
                "recall": 0.983739837398374,
                "f1-score": 0.9918032786885246,
                "support": 123
            },
            "org:top_members/employees": {
                "precision": 0.9784172661870504,
                "recall": 1.0,
                "f1-score": 0.989090909090909,
                "support": 136
            },
            "per:age": {
                "precision": 1.0,
                "recall": 0.9893617021276596,
                "f1-score": 0.9946524064171123,
                "support": 94
            },
            "org:alternate_names": {
                "precision": 1.0,
                "recall": 1.0,
                "f1-score": 1.0,
                "support": 77
            },
            "accuracy": 0.9943502824858758,
            "macro avg": {
                "precision": 0.9949640287769785,
                "recall": 0.9939008834447751,
                "f1-score": 0.9943898943788774,
                "support": 708
            },
            "weighted avg": {
                "precision": 0.9944417347477951,
                "recall": 0.9943502824858758,
                "f1-score": 0.9943580411274868,
                "support": 708
            }
        }
    
}}
